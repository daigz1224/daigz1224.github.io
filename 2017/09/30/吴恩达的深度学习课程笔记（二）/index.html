<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>吴恩达的深度学习课程笔记（二） | DDay's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">吴恩达的深度学习课程笔记（二）</h1><a id="logo" href="/.">DDay's blog</a><p class="description"> GET HANDS DIRTY AND MAKE IT PRETTY.</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">吴恩达的深度学习课程笔记（二）</h1><div class="post-meta">Sep 30, 2017<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Week-1-Setting-up-your-ML-Application"><span class="toc-number">1.</span> <span class="toc-text">Week 1. Setting up your ML Application</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Train-Dev-Test-Sets"><span class="toc-number">1.1.</span> <span class="toc-text">Train / Dev / Test Sets</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bias-Variance-Tradeoff"><span class="toc-number">1.2.</span> <span class="toc-text">Bias / Variance Tradeoff</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Basic-“Recipe”-for-ML"><span class="toc-number">1.3.</span> <span class="toc-text">Basic “Recipe” for ML</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Regularization"><span class="toc-number">1.4.</span> <span class="toc-text">Regularization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Normalizing-Inputs"><span class="toc-number">1.5.</span> <span class="toc-text">Normalizing Inputs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Vanishing-Exploding-Gradients"><span class="toc-number">1.6.</span> <span class="toc-text">Vanishing / Exploding Gradients</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Gradient-Check"><span class="toc-number">1.7.</span> <span class="toc-text">Gradient Check</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Week-2-Optimization-Algorithms"><span class="toc-number">2.</span> <span class="toc-text">Week 2. Optimization Algorithms</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Mini-batch-Gradient-Descent"><span class="toc-number">2.1.</span> <span class="toc-text">Mini-batch Gradient Descent</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Exponentially-Weighted-Averages"><span class="toc-number">2.2.</span> <span class="toc-text">Exponentially Weighted Averages</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Momentum"><span class="toc-number">2.3.</span> <span class="toc-text">Momentum</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RMSprop"><span class="toc-number">2.4.</span> <span class="toc-text">RMSprop</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Adam"><span class="toc-number">2.5.</span> <span class="toc-text">Adam</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Learning-Rate-Decay"><span class="toc-number">2.6.</span> <span class="toc-text">Learning Rate Decay</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Problem-of-Local-Optima"><span class="toc-number">2.7.</span> <span class="toc-text">The Problem of Local Optima</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Week-3-Hyper-Parameter-Tuning"><span class="toc-number">3.</span> <span class="toc-text">Week 3. Hyper Parameter Tuning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Tuning-Process"><span class="toc-number">3.1.</span> <span class="toc-text">Tuning Process</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Appropriate-Scale-to-Pick"><span class="toc-number">3.2.</span> <span class="toc-text">Appropriate Scale to Pick</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Batch-Normalization"><span class="toc-number">3.3.</span> <span class="toc-text">Batch Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Softmax-Regression"><span class="toc-number">3.4.</span> <span class="toc-text">Softmax Regression</span></a></li></ol></li></ol></div></div><div class="post-content"><p>本系列的目标是将 Andrew Ng 的 Deep Learning 课程整体梳理一遍，读薄课程中的基础知识和关键概念，便于回顾。文章内容大部分来自我的手写笔记，中英文混杂还请见谅。这个系列一共有五门课程，本文是本系列的第二课：<strong>Improving Deep Neural Network: Hyperparameter tuning, Regularization, Optimization</strong></p>
<a id="more"></a>
<p>大家好，我是 Day。听过很多道理，却依然过不好这一生。看过很多书和视频，却与进阶知识眉来眼去不敢向前。前段时间读了一个非常好的个人博客，<a href="http://wdxtub.com/" target="_blank" rel="noopener">小土刀</a>，受益匪浅，他将看过的书都整理了下来，即所谓的”读薄”，沉淀下来总是最珍贵的。</p>
<hr>
<h3 id="Week-1-Setting-up-your-ML-Application"><a href="#Week-1-Setting-up-your-ML-Application" class="headerlink" title="Week 1. Setting up your ML Application"></a>Week 1. Setting up your ML Application</h3><h4 id="Train-Dev-Test-Sets"><a href="#Train-Dev-Test-Sets" class="headerlink" title="Train / Dev / Test Sets"></a>Train / Dev / Test Sets</h4><ul>
<li>Train Set：用训练集对算法或模型进行训练过程；</li>
<li>Development Set：利用验证集或者又称为简单交叉验证集（Hold-out Cross Validation Set）进行交叉验证，选择出最好的模型；</li>
<li>Test Set：最后利用测试集对模型进行测试，获取模型运行的无偏估计。</li>
</ul>
<p><strong>小数据集</strong>：如 100, 1000, 10000 的数据量大小，可以将数据集做以下划分：</p>
<ul>
<li>无验证集的情况：70% / 30%；</li>
<li>有验证集的情况：60% / 20% / 20%；</li>
</ul>
<p><strong>大数据集</strong>：百万级别的数据量，可以选择 1000 条数据就足以评估单个模型效果，可以将数据集做以下划分：</p>
<ul>
<li>100万数据量：98% / 1% / 1%；</li>
<li>超百万数据量：99.5% / 0.25% / 0.25%（or 99.5% / 0.4% / 0.1%）</li>
</ul>
<p><strong>建议：</strong></p>
<ul>
<li>验证集要和训练集来自于同一个分布（Shuffle 一下），可以使得机器学习算法变得更快；</li>
<li>如果不需要用无偏估计来评估模型的性能，则可以不需要测试集。</li>
</ul>
<h4 id="Bias-Variance-Tradeoff"><a href="#Bias-Variance-Tradeoff" class="headerlink" title="Bias / Variance Tradeoff"></a>Bias / Variance Tradeoff</h4><p>The base error is quite small, e.g Human-level ≈ 0%</p>
<table>
<thead>
<tr>
<th style="text-align:center">Train Set Error</th>
<th style="text-align:center">1%</th>
<th style="text-align:center">15%</th>
<th style="text-align:center">15%</th>
<th style="text-align:center">0.5%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>Dev Set Error</strong></td>
<td style="text-align:center"><strong>11%</strong></td>
<td style="text-align:center"><strong>16%</strong></td>
<td style="text-align:center"><strong>30%</strong></td>
<td style="text-align:center"><strong>1%</strong></td>
</tr>
<tr>
<td style="text-align:center"><strong>Bias / Vaiance</strong></td>
<td style="text-align:center">High Variance</td>
<td style="text-align:center">High Bias</td>
<td style="text-align:center">HB &amp; HV</td>
<td style="text-align:center">LB &amp; LV</td>
</tr>
</tbody>
</table>
<h4 id="Basic-“Recipe”-for-ML"><a href="#Basic-“Recipe”-for-ML" class="headerlink" title="Basic “Recipe” for ML"></a>Basic “Recipe” for ML</h4><p><strong>是否 High Bias（欠拟合）？</strong></p>
<ul>
<li>Bigger Network</li>
<li>Train Longer</li>
<li>(NN Architecture Search)</li>
</ul>
<p><strong>是否 High Variance（过拟合）？</strong></p>
<ul>
<li>More Data</li>
<li>Regularization</li>
<li>(NN Architecture Search)</li>
</ul>
<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p><strong>直观理解</strong>：正则化因子 $\lambda$ 设置的足够大的情况下，为了使代价函数最小化，权重矩阵 $W$就会被设置为接近于 0 的值。则相当于消除了很多神经元的影响，那么图中的大的神经网络就会变成一个较小的网络。实际上隐藏层的神经元依然存在，但是他们的影响变小了，便不会导致过拟合。</p>
<p><strong>L2 </strong> (=Weight Decay):</p>
<p><strong>In Logistic Regression</strong>:</p>
<p>$$J(w,b)=\cfrac{1}{m}\sum\limits^m_{i=1}L(\hat{y}^{(i)},y^{(i)})+\cfrac{\lambda}{2m}||w||^2_2$$</p>
<ul>
<li><p>L2 Regularization:</p>
<p> $$\cfrac{\lambda}{2m}||w||^2_2=\cfrac{\lambda}{2m}\sum\limits^{n<em>x}</em>{j=1}w^2_j=\cfrac{\lambda}{2m}w^Tw$$</p>
</li>
<li><p>L1 Regularization: </p>
<p>$$\cfrac{\lambda}{2m}\mid\mid w\mid\mid_1=\cfrac{\lambda}{2m}\sum\limits^{n<em>x}</em>{j=1}\mid w_j\mid$$</p>
</li>
</ul>
<p><strong>In Neural Network</strong>:</p>
<p>$$J(w^{[1]},b^{[1]},…,w^{[L]},b^{[L]},)=\cfrac{1}{m}\sum\limits^m<em>{i=1}L(\hat{y}^{(i)},y^{(i)})+\cfrac{\lambda}{2m}\sum\limits^L</em>{l=1}||w^{l}||^2_F$$</p>
<p>$w.shape = (n^{[l]},n^{[l-1]})$, $F$ is “Frobenius norm”</p>
<p><strong>Weight Decay</strong>:</p>
<p>$$dW^{[l]}=(from\,backprop)+\cfrac{\lambda}{m}W^{[l]}$$</p>
<p>$$W^{[l]}:=W^{[l]}-\alpha dW^{[l]}=(1-\cfrac{\alpha\lambda}{m})W^{[l]}-\alpha(from\,backprop)$$</p>
<p>$Notes: (1-\cfrac{\alpha\lambda}{m})&lt;1$</p>
<p><strong>数学原理</strong>：假设神经元中使用的激活函数为 $g(z) = tanh(z)$ 在加入正则化项后：当 $\lambda$ 增大，导致 $W^{[l]}$ 减小，$Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}$ 便会减小，当   处于$z$数值较小的区域里，$tanh(z)$ 函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，从而不会发生过拟合。</p>
<p><strong>Dropout</strong> -“Inverted dropout”</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># layer L = 3, keep_prob = 0.8</span><br><span class="line">d3 = np.random.rand(a3.shape[0], a3.shape[1]) &lt; keep_prob</span><br><span class="line">a3 = np.multiply(a3, d3)</span><br><span class="line">a3 /= keep_prob  # 为了不影响Z[4] = W[4]*a[3]+b[4]的期望值</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：在测试阶段不要用 dropout，因为那样会使得预测结果变得随机。</p>
<p><strong>直观理解</strong>：通过 dropout，使得网络不会依赖于任何一个特征（可能会被丢弃），从而 shrink the weights。</p>
<p><strong>缺点</strong>：使得 Cost function 不再能被明确的定义。</p>
<p><strong>使用</strong>：关闭 dropout 功能，即设置 keep_prob = 1.0；运行代码，确保 Cost Function 单调递减；再打开 dropout。</p>
<p><strong>Data Augmentation</strong></p>
<p>数据扩增，通过图片的一些变换，得到更多的训练集和验证集。</p>
<p><strong>Early Stopping</strong></p>
<p>在交叉验证集的误差上升之前的点停止迭代，避免过拟合。这种方法的缺点是无法同时解决 bias 和 variance 之间的最优。</p>
<h4 id="Normalizing-Inputs"><a href="#Normalizing-Inputs" class="headerlink" title="Normalizing Inputs"></a>Normalizing Inputs</h4><p>$$\mu=\cfrac{1}{m}\sum\limits^{[m]}_{i=1}x^{(i)}$$</p>
<p>$$\sigma^{2}=\cfrac{1}{m}\sum\limits^m_{i=1}x^{(i)^2}$$</p>
<p>$$x:=x-\mu,x=x/\sigma^2$$</p>
<p>在不使用归一化的代价函数中，如果我们设置一个较小的学习率，那么很可能我们需要很多次迭代才能到达代价函数全局最优解；如果使用了归一化，那么无论从哪个位置开始迭代，我们都能以相对很少的迭代次数找到全局最优解。</p>
<h4 id="Vanishing-Exploding-Gradients"><a href="#Vanishing-Exploding-Gradients" class="headerlink" title="Vanishing / Exploding Gradients"></a>Vanishing / Exploding Gradients</h4><p>在梯度函数上出现的以指数级递增或者递减的情况就分别称为梯度爆炸或者梯度消失。</p>
<p><strong>减缓梯度爆炸或梯度消失</strong>：</p>
<p>当输入的 $x^{(i)}$ 的维度较大时，我们希望每个 $w$ 都小一点，这样得到的和 $z$ 也较小。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Xavier Initialization</span><br><span class="line">WL = np.random.randn(WL.shape[0],WL.shape[1])* np.sqrt(1/n)</span><br><span class="line"># 激活函数使用 ReLU 的话，使用 np.sqrt(2/n)</span><br><span class="line"># 激活函数使用 tanh 的话，使用 np.sqrt(1/n)</span><br></pre></td></tr></table></figure>
<p>这么做是因为，如果激活函数的输入 $x^{(i)}$ 近似设置成均值为 0，标准方差 1 的情况，输出 $z$ 也会调整到相似的范围内。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。</p>
<h4 id="Gradient-Check"><a href="#Gradient-Check" class="headerlink" title="Gradient Check"></a>Gradient Check</h4><p>使用双边误差的方法逼近导数，误差为 $O(\epsilon^2)$，单边为$O(\epsilon)$</p>
<p>因为我们的神经网络中含有大量的参数: $W[1], b[1], ⋯, W[L], b[L]$，为了做梯度检验，需要将这些参数全部连接起来，reshape 成一个大的向量 $\Theta$ 。同时对 $dW[1],db[1],⋯,dW[L],db[L]$ 执行同样的操作 $d\Theta$。 </p>
<p>$$d\Theta_{approx}[i]=\cfrac{J(\theta_1,…,\theta_i+\epsilon,…)-J(\theta_1,…,\theta_i-\epsilon,…)}{2\epsilon}$$</p>
<p>由，$\cfrac{||d\theta_{approx}−d\theta||<em>2}{||d\theta</em>{approx}||_2+||d\theta||_2}$</p>
<p>判断是否 $d\Theta_{approx}≈d\Theta$</p>
<ul>
<li>不要在训练过程中使用梯度检验，只在 debug 的时候使用，使用完毕关闭梯度检验的功能；</li>
<li>如果算法的梯度检验出现了错误，要检查每一项，找出错误，也就是说要找出哪个 $d\Theta_{approx}[i]​$ 与 $d\Theta​$ 的值相差比较大；</li>
<li>不要忘记了正则化项；</li>
<li>梯度检验不能与 dropout 同时使用。因为每次迭代的过程中，dropout 会随机消除隐层单元的不同神经元，这时是难以计算 dropout 在梯度下降上的代价函数 $J$；</li>
<li>在随机初始化的时候运行梯度检验，或许在训练几次后再进行。</li>
</ul>
<hr>
<h3 id="Week-2-Optimization-Algorithms"><a href="#Week-2-Optimization-Algorithms" class="headerlink" title="Week 2. Optimization Algorithms"></a>Week 2. Optimization Algorithms</h3><h4 id="Mini-batch-Gradient-Descent"><a href="#Mini-batch-Gradient-Descent" class="headerlink" title="Mini-batch Gradient Descent"></a>Mini-batch Gradient Descent</h4><ul>
<li>If mini-batch size = m: Batch Gradient Descent</li>
<li>If mini-batch size = 1: Stochastic Gradient Descent</li>
<li>In-Between: fastest<ul>
<li>Vectorization </li>
<li>Make progress without processing the entire set.</li>
</ul>
</li>
<li>If small training set (m≤2000): use Batch Gradient Descent</li>
<li>Typical mini-batch size: 64, 128, 256, 512</li>
<li>Make sure mini-batch fit in CPU / GPU memory</li>
</ul>
<h4 id="Exponentially-Weighted-Averages"><a href="#Exponentially-Weighted-Averages" class="headerlink" title="Exponentially Weighted Averages"></a>Exponentially Weighted Averages</h4><p>$$V<em>t=\beta V</em>{t-1}+(1-\beta)\theta_t$$</p>
<p>此时 $V_t$ 相当于之前 $\cfrac{1}{1-\beta}$ 个数据的平均。</p>
<p><strong>Bias Correction</strong>: 因为冷启动的问题，所以可以用 $\cfrac{V_t}{1-\beta^t}=\cfrac{\beta V_t+(1-\beta)\theta_t}{1-\beta^t}$ 进行偏差修正。</p>
<h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><p><strong>直观理解</strong>：选择了一条更直接的路径，减缓梯度的下降幅度。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Hyperparameters: alpha, beta</span><br><span class="line"># beta = 0.9, 相当于前10次平均，无需bias correction</span><br><span class="line">VdW = 0; Vdb = 0</span><br><span class="line">on iteration t:</span><br><span class="line">	compute dW, db on current mini-batch</span><br><span class="line">	VdW = beta*VdW + (1-beta)*dW</span><br><span class="line">	Vdb = beta*Vdb + (1-beta)*db</span><br><span class="line">	W := W - alpha*VdW</span><br><span class="line">	b := b - alpha*db</span><br></pre></td></tr></table></figure>
<h4 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h4><p>控制 $W, b$ 的梯度下降速度。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Hyperparameters: alpha, beta</span><br><span class="line">SdW = 0; Sdb = 0</span><br><span class="line">on iteration t:</span><br><span class="line">	compute dW, db on current mini-batch</span><br><span class="line">	SdW = beta*SdW + (1-beta)*sqrt(dW,2)</span><br><span class="line">	Sdb = beta*Sdb + (1-beta)*sqrt(db,2)</span><br><span class="line">	W := W - alpha*dW/sqrt(SdW,0.5)  # SdW越小，W变化就越大</span><br><span class="line">	b := b - alpha*db/sqrt(SdW,0.5)  # Sdb越大，b变化就越大</span><br></pre></td></tr></table></figure>
<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># Hyperparameters: alpha, beta1, beta2, epsilon</span><br><span class="line">VdW = 0; Vdb = 0; SdW = 0; Sdb = 0</span><br><span class="line">on iteration t:</span><br><span class="line">	compute dW, db on current mini-batch</span><br><span class="line">	</span><br><span class="line">	VdW = beta1*VdW + (1-beta1)*dW</span><br><span class="line">	Vdb = beta1*Vdb + (1-beta1)*db</span><br><span class="line">	SdW = beta2*SdW + (1-beta2)*sqrt(dW,2)</span><br><span class="line">	Sdb = beta2*Sdb + (1-beta2)*sqrt(db,2)</span><br><span class="line">	</span><br><span class="line">	VdW_correction = VdW / (1-sqrt(beta1,t))</span><br><span class="line">	Vdb_correction = Vdb/ (1-sqrt(beta1,t))</span><br><span class="line">	SdW_correction = SdW / (1-sqrt(beta2,t))</span><br><span class="line">	Sdb_correction = Sdb/ (1-sqrt(beta2,t))</span><br><span class="line">	</span><br><span class="line">	W := W - alpha*(VdW_correction/(sqrt(SdW_correction,0.5)+epsilon))</span><br><span class="line">	b := b - alpha*(Vdb_correction/(sqrt(Sdb_correction,0.5)+epsilon))</span><br></pre></td></tr></table></figure>
<h4 id="Learning-Rate-Decay"><a href="#Learning-Rate-Decay" class="headerlink" title="Learning Rate Decay"></a>Learning Rate Decay</h4><p>$$\alpha=\cfrac{1}{1+decay<em>{rate}epoch</em>{num}}\alpha_0$$</p>
<h4 id="The-Problem-of-Local-Optima"><a href="#The-Problem-of-Local-Optima" class="headerlink" title="The Problem of Local Optima"></a>The Problem of Local Optima</h4><ul>
<li>Unlikely to get stuck in a bad local optima, usually in saddle point.</li>
<li>The plateaus can make learning slow.</li>
</ul>
<hr>
<h3 id="Week-3-Hyper-Parameter-Tuning"><a href="#Week-3-Hyper-Parameter-Tuning" class="headerlink" title="Week 3. Hyper Parameter Tuning"></a>Week 3. Hyper Parameter Tuning</h3><h4 id="Tuning-Process"><a href="#Tuning-Process" class="headerlink" title="Tuning Process"></a>Tuning Process</h4><p>在一定范围内随机取参数，不要使用网格记录，要由粗到细。</p>
<ul>
<li>$\alpha$</li>
<li>$\beta (=0.9)$, num of hidden units, mini-batch size</li>
<li>num of layers, learning rate decay</li>
<li>$\beta_1 (=0.9)$, $\beta_2 (=0.999)$, $\epsilon (=10^{-8})$</li>
</ul>
<h4 id="Appropriate-Scale-to-Pick"><a href="#Appropriate-Scale-to-Pick" class="headerlink" title="Appropriate Scale to Pick"></a>Appropriate Scale to Pick</h4><p>不能线性取值，因为不同区段敏感度不同。</p>
<p>如果要选 $\alpha$ = 0.001 ~ 0.1, 那么就随机在 -4 到 -1选一个 $r$，则 $\alpha = 10^r$。</p>
<p>如果要选 $\beta$  = 0.9 ~ 0.999, 那么就转换为 $1 - \beta$ 的及进行选择。 </p>
<p>要用鱼子酱（Caviar）模式去试，即 Training many models in parallel，而不是 Panda 模式，即 Baby-sitting one model。</p>
<h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>Given some intermediate values in NN, like $Z^{<a href="1">l</a>},Z^{<a href="2">l</a>},…,Z^{<a href="m">l</a>}$</p>
<p>$$\mu=\cfrac{1}{m}\sum\limits_iZ^{(i)}$$</p>
<p>$$\sigma^2=\cfrac{1}{m}\sum\limits_i(Z^{(i)}-\mu)^2$$</p>
<p>$$Z^{(i)}_{norm}=\cfrac{Z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}$$</p>
<p>$$\hat{Z}^{(i)}=\gamma Z^{(i)}_{norm}+\beta$$</p>
<p>In this layer, Use $\hat{Z}^{(i)}$ instead of $Z^{(i)}$, then enter the activation function.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for t = 1,...,num of mini-batches:</span><br><span class="line">	compute FP on X&#123;t&#125;,</span><br><span class="line">		In each hidden layer, use Batch Normalization to replace Z[l] with Z[l]_hat.</span><br><span class="line">	Use BP to compute dW[l], dbeta[l], dgamma[l].(no need for db[l])</span><br><span class="line">	Update parameters: </span><br><span class="line">		W[l] := W[l] - alpha*dW[l]</span><br><span class="line">		beta[l] := beta[l] - alpha*dbeta[l]</span><br><span class="line">		gamma[l] := gamma[l] - alpha*dgamma[l]</span><br><span class="line">(can work with/without momentum/RMSprop/Adam)</span><br></pre></td></tr></table></figure>
<p>Batch Normalization 使每层参数稍稍独立于其他层，这样后面的层对前面层的参数不那么敏感。</p>
<p><strong>Batch Normalization as Regularization</strong>:</p>
<ol>
<li>Each mini-batch is scaled by the mean/Variance.</li>
<li>Add some noise to the value Z[l], similarly to dropout.</li>
<li>This has a slight regularization effect.</li>
</ol>
<p><strong>Notes</strong>:</p>
<ul>
<li>训练时，$\mu$ 和 $\sigma^2$是根据 mini-batch 来计算的。即通过64，128，…个样本的计算。</li>
<li>测试时，需要逐一处理样本，所以需要估算。</li>
<li>可以通过指数加权平均的方法估算，即 Exponetically Weighted Average (across mini-batch).</li>
</ul>
<h4 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h4><p>多分类任务，通过最后一层”激活函数”（Softmax Layer）来计算每个分类的概率。</p>
<p>$$A^{[L][i]} = \cfrac{e^{Z^{[L][i]}}}{\sum\limits_iZ^{[L]}}$$</p>
</div><div class="tags"><a href="/tags/深度学习/">深度学习</a><a href="/tags/吴恩达/">吴恩达</a></div><div class="post-nav"><a href="/2017/09/30/吴恩达的深度学习课程笔记（三)/" class="pre">吴恩达的深度学习课程笔记（三）</a><a href="/2017/09/30/吴恩达的深度学习课程笔记（一）/" class="next">吴恩达的深度学习课程笔记（一）</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><a href="http://dday.top/">
<img src="http://owinowxgh.bkt.clouddn.com/meblack2.jpg" alt="" border="0" style="margin-top:15px; border-radius: 300px;" width="180px"; height="180px"; >
</a></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目整理/">项目整理</a><span class="category-list-count">3</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/tags/MXNet/" style="font-size: 15px;">MXNet</a> <a href="/tags/Gluon/" style="font-size: 15px;">Gluon</a> <a href="/tags/AWS/" style="font-size: 15px;">AWS</a> <a href="/tags/Ubuntu/" style="font-size: 15px;">Ubuntu</a> <a href="/tags/PyTorch/" style="font-size: 15px;">PyTorch</a> <a href="/tags/竞赛/" style="font-size: 15px;">竞赛</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/NexT/" style="font-size: 15px;">NexT</a> <a href="/tags/GitHub/" style="font-size: 15px;">GitHub</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/科学上网/" style="font-size: 15px;">科学上网</a> <a href="/tags/Shadowsocks/" style="font-size: 15px;">Shadowsocks</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/理论/" style="font-size: 15px;">理论</a> <a href="/tags/吴恩达/" style="font-size: 15px;">吴恩达</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/C/" style="font-size: 15px;">C++</a> <a href="/tags/conda/" style="font-size: 15px;">conda</a> <a href="/tags/ffmpeg/" style="font-size: 15px;">ffmpeg</a> <a href="/tags/Windows/" style="font-size: 15px;">Windows</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/01/17/损失函数的推导/">损失函数的推导</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/15/Gluon for MXNet 使用与概念梳理（冗长版）/">Gluon for MXNet 使用与概念梳理（冗长版）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/14/IPV4 IPV6 科学上网/">IPV4 IPV6 科学上网</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/14/激活函数一览/">激活函数一览</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/09/快排，归并，堆排序算法/">快排，归并，堆排序算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/05/终端命令整理/">终端命令整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/01/Kaggle - 120 种狗的图像识别/">Kaggle - 120种狗的图像识别</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/13/AWS 上使用 MXNet 的环境配置/">AWS 上使用 MXNet 的环境配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/15/AI Challenger 场景分类/">AI Challenger 场景分类</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/07/Ubuntu 下 PyTorch 的环境配置/">Ubuntu 下 PyTorch 的环境配置</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">DDay's blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css"><script type="text/javascript" color="100,99,98" opacity="0.5" zIndex="-2" count="50" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>