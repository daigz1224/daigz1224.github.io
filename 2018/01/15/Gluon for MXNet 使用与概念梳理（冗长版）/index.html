<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Gluon for MXNet 使用与概念梳理（冗长版） | DDay's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Gluon for MXNet 使用与概念梳理（冗长版）</h1><a id="logo" href="/.">DDay's blog</a><p class="description"> GET HANDS DIRTY AND MAKE IT PRETTY.</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Gluon for MXNet 使用与概念梳理（冗长版）</h1><div class="post-meta">Jan 15, 2018<span> | </span><span class="category"><a href="/categories/工具/">工具</a></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-安装使用"><span class="toc-number">1.</span> <span class="toc-text">1. 安装使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-预备知识"><span class="toc-number">2.</span> <span class="toc-text">2. 预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-NDArray"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 NDArray</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-autograd"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 autograd</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-入门使用"><span class="toc-number">3.</span> <span class="toc-text">3. 入门使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-线性回归"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-多类逻辑回归"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 多类逻辑回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-多层感知机"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 多层感知机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-L2-正则化和-Dropout"><span class="toc-number">3.4.</span> <span class="toc-text">3.4 L2 正则化和 Dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-批量归一化"><span class="toc-number">3.5.</span> <span class="toc-text">3.5 批量归一化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-各种CNN网络结构"><span class="toc-number">4.</span> <span class="toc-text">4. 各种CNN网络结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-卷积神经网络"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-卷积层"><span class="toc-number">4.1.1.</span> <span class="toc-text">4.1.1 卷积层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-池化层"><span class="toc-number">4.1.2.</span> <span class="toc-text">4.2.2 池化层</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-LeNet"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 LeNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-AlexNet"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 AlexNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-VGG"><span class="toc-number">4.4.</span> <span class="toc-text">4.4 VGG</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-NiN"><span class="toc-number">4.5.</span> <span class="toc-text">4.5 NiN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-GoogLeNet"><span class="toc-number">4.6.</span> <span class="toc-text">4.6 GoogLeNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-ResNet"><span class="toc-number">4.7.</span> <span class="toc-text">4.7 ResNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-DenseNet"><span class="toc-number">4.8.</span> <span class="toc-text">4.8 DenseNet</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Gluon-基础"><span class="toc-number">5.</span> <span class="toc-text">5. Gluon 基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-自定义神经网络"><span class="toc-number">5.1.</span> <span class="toc-text">5.1 自定义神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-模型参数访问与初始化"><span class="toc-number">5.2.</span> <span class="toc-text">5.2 模型参数访问与初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-序列化-读写模型"><span class="toc-number">5.3.</span> <span class="toc-text">5.3 序列化 - 读写模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-GPU的使用"><span class="toc-number">5.4.</span> <span class="toc-text">5.4 GPU的使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-优化算法"><span class="toc-number">6.</span> <span class="toc-text">6. 优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-SGD"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 SGD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-Momentum"><span class="toc-number">6.2.</span> <span class="toc-text">6.2 Momentum</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-Adagrad"><span class="toc-number">6.3.</span> <span class="toc-text">6.3 Adagrad</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-RMSProp"><span class="toc-number">6.4.</span> <span class="toc-text">6.4 RMSProp</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-Adadelta"><span class="toc-number">6.5.</span> <span class="toc-text">6.5 Adadelta</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-6-Adam"><span class="toc-number">6.6.</span> <span class="toc-text">6.6 Adam</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-进阶操作"><span class="toc-number">7.</span> <span class="toc-text">7. 进阶操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-Hybridize"><span class="toc-number">7.1.</span> <span class="toc-text">7.1 Hybridize</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-延后执行"><span class="toc-number">7.2.</span> <span class="toc-text">7.2 延后执行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-自动并行"><span class="toc-number">7.3.</span> <span class="toc-text">7.3 自动并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-多-CPU-训练"><span class="toc-number">7.4.</span> <span class="toc-text">7.4 多 CPU 训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-计算机视觉领域的常用操作"><span class="toc-number">8.</span> <span class="toc-text">8. 计算机视觉领域的常用操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-图片增广"><span class="toc-number">8.1.</span> <span class="toc-text">8.1 图片增广</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-Fine-tuning"><span class="toc-number">8.2.</span> <span class="toc-text">8.2 Fine-tuning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文章"><span class="toc-number">9.</span> <span class="toc-text">参考文章</span></a></li></ol></div></div><div class="post-content"><p>基本是按照官方 Gluon 中文教程整理。</p>
<a id="more"></a>
<h2 id="1-安装使用"><a href="#1-安装使用" class="headerlink" title="1. 安装使用"></a>1. 安装使用</h2><ul>
<li><a href="http://dday.top/2017/12/13/AWS%20%E4%B8%8A%E4%BD%BF%E7%94%A8%20MXNet%20%E7%9A%84%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" target="_blank" rel="noopener">AWS 上使用 MXNet 的环境配置</a></li>
</ul>
<h2 id="2-预备知识"><a href="#2-预备知识" class="headerlink" title="2. 预备知识"></a>2. 预备知识</h2><h3 id="2-1-NDArray"><a href="#2-1-NDArray" class="headerlink" title="2.1 NDArray"></a>2.1 NDArray</h3><ul>
<li>数据处理包括数据读取以及数据已经在内存中时如何处理。</li>
<li><code>NDArray ≈ NumPy + CPU/GPU异步计算 + 自动求导</code></li>
<li>mxnet.ndarray 的基本操作：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</span><br><span class="line"><span class="comment">## 创建矩阵</span></span><br><span class="line">x = nd.ones((<span class="number">3</span>, <span class="number">4</span>)); x = nd.zeros((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">y = nd.random_normal(<span class="number">0</span>, <span class="number">1</span>, shape=(<span class="number">3</span>, <span class="number">4</span>)) <span class="comment">#服从均值0标准差1的正态分布</span></span><br><span class="line">x.shape; x.size</span><br><span class="line"><span class="comment">## 操作符</span></span><br><span class="line">z = x + y; z = x * y; z = nd.exp(y)</span><br><span class="line">z = nd.dot(x, y.T)</span><br><span class="line"><span class="comment">## 广播</span></span><br><span class="line">a = nd.arange(<span class="number">3</span>).reshape((<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">b = nd.arange(<span class="number">2</span>).reshape((<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">c = a + b <span class="comment">#c.shape == (3,2)</span></span><br><span class="line"><span class="comment">## 与NumPy的互转</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.ones((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">y = nd.array(x)  <span class="comment"># numpy -&gt; mxnet.ndarray</span></span><br><span class="line">z = y.asnumpy()  <span class="comment"># mxnet.ndarray -&gt; numpy</span></span><br><span class="line"><span class="comment">## 需要新开内存的操作</span></span><br><span class="line">y = y + x</span><br><span class="line">z = nd.zeros_like(x); z[:] = x + y <span class="comment">#有临时空间的使用</span></span><br><span class="line"><span class="comment">## 避免内存开销</span></span><br><span class="line">y += x; y[:] = y + x</span><br><span class="line">z = nd.zeros_like(x); nd.elemwise_add(x, y, out=z)</span><br><span class="line"><span class="comment">## 截取 slicing</span></span><br><span class="line">x = nd.arange(<span class="number">0</span>,<span class="number">9</span>).reshape((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">x[<span class="number">1</span>:<span class="number">3</span>] <span class="comment">#截取第1，2行</span></span><br><span class="line">x[<span class="number">1</span>:<span class="number">2</span>,<span class="number">1</span>:<span class="number">3</span>] <span class="comment"># 多维截取，第2行的第2，3列</span></span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://mxnet.incubator.apache.org/api/python/ndarray.html" target="_blank" rel="noopener">NDArray API</a></li>
</ul>
<h3 id="2-2-autograd"><a href="#2-2-autograd" class="headerlink" title="2.2 autograd"></a>2.2 autograd</h3><ul>
<li><code>mxnet.autograd</code> 可以对正常的命令式程序进行求导。它每次在后端实时创建计算图，从而可以立即得到梯度的计算方法。</li>
<li><code>mxnet.autograd</code> 的基本操作：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mxnet.ndarray <span class="keyword">as</span> nd</span><br><span class="line"><span class="keyword">import</span> mxnet.autograd <span class="keyword">as</span> ag</span><br><span class="line">x = nd.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="comment">## 申请存放导数的空间</span></span><br><span class="line">x.attach_grad()</span><br><span class="line"><span class="comment">## 显示要求记录求导的程序</span></span><br><span class="line"><span class="keyword">with</span> ag.record():</span><br><span class="line">    y = x * <span class="number">2</span></span><br><span class="line">    z = y * x</span><br><span class="line"><span class="comment">## 求导</span></span><br><span class="line">z.backward() <span class="comment">#如果z不是一个标量，那么z.backward()等价于nd.sum(z).backward().</span></span><br><span class="line"><span class="comment">## 查看导数</span></span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>
<ul>
<li>autograd 可以对动态图自动求导。</li>
</ul>
<h2 id="3-入门使用"><a href="#3-入门使用" class="headerlink" title="3. 入门使用"></a>3. 入门使用</h2><h3 id="3-1-线性回归"><a href="#3-1-线性回归" class="headerlink" title="3.1 线性回归"></a>3.1 线性回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 创建数据集</span></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, <span class="number">-3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">X = nd.random_normal(shape=(num_examples, num_inputs))</span><br><span class="line">y = true_w[<span class="number">0</span>] * X[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * X[:, <span class="number">1</span>] + true_b</span><br><span class="line">y += <span class="number">.01</span> * nd.random_normal(shape=y.shape)</span><br><span class="line"><span class="comment">## 数据读取</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">dataset = gluon.data.ArrayDataset(X, y)</span><br><span class="line">data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">## 定义模型</span></span><br><span class="line">net = gluon.nn.Sequential()</span><br><span class="line">net.add(gluon.nn.Dense(<span class="number">1</span>)) <span class="comment">#需要指定输出节点的个数</span></span><br><span class="line">net.initialize() <span class="comment">#模型初始化</span></span><br><span class="line">square_loss = gluon.loss.L2Loss() <span class="comment">#定义损失函数</span></span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;) <span class="comment">#优化算法，学习率</span></span><br><span class="line"><span class="comment">## 训练</span></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            output = net(data)</span><br><span class="line">            loss = square_loss(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line">        total_loss += nd.sum(loss).asscalar()</span><br><span class="line">    print(<span class="string">"Epoch %d, average loss: %f"</span> % (e, total_loss/num_examples))</span><br><span class="line"><span class="comment">## 查看weight和bias</span></span><br><span class="line">dense = net[<span class="number">0</span>]</span><br><span class="line">dense.weight.data()</span><br><span class="line">dense.bias.data()</span><br></pre></td></tr></table></figure>
<h3 id="3-2-多类逻辑回归"><a href="#3-2-多类逻辑回归" class="headerlink" title="3.2 多类逻辑回归"></a>3.2 多类逻辑回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> ndarray <span class="keyword">as</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 获取 gluon.data.vision.FashionMNIST 数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(data, label)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> data.astype(<span class="string">'float32'</span>)/<span class="number">255</span>, label.astype(<span class="string">'float32'</span>)</span><br><span class="line">mnist_train = gluon.data.vision.FashionMNIST(train=<span class="keyword">True</span>, transform=transform)</span><br><span class="line">mnist_test = gluon.data.vision.FashionMNIST(train=<span class="keyword">False</span>, transform=transform)</span><br><span class="line"><span class="comment">## 数据读取</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment">## 定义模型</span></span><br><span class="line">net = gluon.nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(gluon.nn.Flatten())</span><br><span class="line">    net.add(gluon.nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize()</span><br><span class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss() <span class="comment">#Softmax和交叉熵的结合</span></span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>&#125;) <span class="comment">#优化算法，学习率</span></span><br><span class="line"><span class="comment">## 计算精度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(output, label)</span>:</span> <span class="comment">#将预测概率最高的那个类作为预测的类，比较</span></span><br><span class="line">    <span class="keyword">return</span> nd.mean(output.argmax(axis=<span class="number">1</span>)==label).asscalar()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span><span class="params">(data_iterator, net)</span>:</span> <span class="comment">#评估模型在数据集上的精度</span></span><br><span class="line">    acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iterator:</span><br><span class="line">        output = net(data)</span><br><span class="line">        acc += accuracy(output, label)</span><br><span class="line">    <span class="keyword">return</span> acc / len(data_iterator)</span><br><span class="line"><span class="comment">## 训练</span></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">    train_loss = <span class="number">0.</span></span><br><span class="line">    train_acc = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> train_data:</span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            output = net(data)</span><br><span class="line">            loss = softmax_cross_entropy(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        trainer.step(batch_size)</span><br><span class="line"> </span><br><span class="line">        train_loss += nd.mean(loss).asscalar()</span><br><span class="line">        train_acc += accuracy(output, label) <span class="comment">#计算训练精度</span></span><br><span class="line"> </span><br><span class="line">    test_acc = evaluate_accuracy(test_data, net) <span class="comment"># 每个epoch就在测试集上评估一下模型精度</span></span><br><span class="line">    print(<span class="string">"Epoch %d. Loss: %f, Train acc %f, Test acc %f"</span> % (</span><br><span class="line">        epoch, train_loss/len(train_data), train_acc/len(train_data), test_acc))</span><br><span class="line"><span class="comment">## 预测</span></span><br><span class="line">data, label = mnist_test[<span class="number">0</span>:<span class="number">9</span>]</span><br><span class="line">predicted_labels = net(data).argmax(axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="3-3-多层感知机"><a href="#3-3-多层感知机" class="headerlink" title="3.3 多层感知机"></a>3.3 多层感知机</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="comment">## 定义模型</span></span><br><span class="line">net = gluon.nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(gluon.nn.Flatten())</span><br><span class="line">    net.add(gluon.nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>)) <span class="comment">#隐含层+ReLU</span></span><br><span class="line">    net.add(gluon.nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize()</span><br><span class="line">softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss() <span class="comment">#损失函数</span></span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.5</span>&#125;) <span class="comment">#优化算法，学习率</span></span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://mxnet.apache.org/api/python/ndarray.html#mxnet.ndarray.Activation" target="_blank" rel="noopener">mxnet.ndarray.Activation</a></li>
</ul>
<h3 id="3-4-L2-正则化和-Dropout"><a href="#3-4-L2-正则化和-Dropout" class="headerlink" title="3.4 L2 正则化和 Dropout"></a>3.4 L2 正则化和 Dropout</h3><ul>
<li>通过优化算法的<code>wd</code>参数（weight decay）实现对模型的正则化。这相当于 L2 范数正则化。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"> </span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;</span><br><span class="line">        <span class="string">'learning_rate'</span>: learning_rate, <span class="string">'wd'</span>: weight_decay&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li>在全连接层后添加<code>gluon.nn.Dropout</code>层并指定元素丢弃概率。</li>
<li>一般情况下，我们推荐把 更靠近输入层的元素丢弃概率设的更小一点.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"> </span><br><span class="line">net = nn.Sequential()</span><br><span class="line">drop_prob1 = <span class="number">0.2</span></span><br><span class="line">drop_prob2 = <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(nn.Flatten())</span><br><span class="line">    net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>)) <span class="comment">#第一层全连接</span></span><br><span class="line">    net.add(nn.Dropout(drop_prob1)) <span class="comment">#在第一层全连接后添加丢弃层</span></span><br><span class="line">    net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>)) <span class="comment">#第二层全连接</span></span><br><span class="line">    net.add(nn.Dropout(drop_prob2)) <span class="comment">#在第二层全连接后添加丢弃层</span></span><br><span class="line">    net.add(nn.Dense(<span class="number">10</span>))</span><br><span class="line">net.initialize()</span><br></pre></td></tr></table></figure>
<h3 id="3-5-批量归一化"><a href="#3-5-批量归一化" class="headerlink" title="3.5 批量归一化"></a>3.5 批量归一化</h3><ul>
<li>如果把目标函数 f 根据参数 w 迭代（如 $f(w−η∇f(w))$ ）进行泰勒展开，有关学习率 η 的高阶项的系数可能由于数量级的原因（通常由于层数多）而不容忽略。然而常用的低阶优化算法（如梯度下降）对于不断降低目标函数的有效性通常基于一个基本假设：在以上泰勒展开中把有关学习率的高阶项通通忽略不计。</li>
<li>在训练时给定一个批量输入，批量归一化试图对深度学习模型的某一层所使用的激活函数的输入进行归一化：使批量呈标准正态分布（均值为0，标准差为1）。</li>
<li>数学推导：</li>
</ul>
<p>$$\mu<em>B \leftarrow \frac{1}{m}\sum\limits</em>{i = 1}^{m}x<em>i,B = {x</em>{1, …, m}}$$</p>
<p>$$\sigma<em>B^2 \leftarrow \frac{1}{m} \sum\limits</em>{i=1}^{m}(x_i - \mu_B)^2$$</p>
<p>$$\hat{x_i} \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$</p>
<p>$$y_i \leftarrow \gamma \hat{x<em>i} + \beta \equiv \mbox{BN}</em>{\gamma,\beta}(x_i)$$</p>
<ul>
<li><code>ndarray</code> 的实现：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="comment">## 全连接层的BN:</span></span><br><span class="line">X = nd.arange(<span class="number">6</span>).reshape((<span class="number">3</span>,<span class="number">2</span>)) <span class="comment">#data: batch_size x features</span></span><br><span class="line">X = X.mean(axis=<span class="number">0</span>)</span><br><span class="line">variance = ((X-mean)**<span class="number">2</span>).mean(axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment">## 二维卷积层的BN</span></span><br><span class="line">X = nd.arrange(<span class="number">6</span>).reshape((<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>)) <span class="comment">#data: batch_size x channels x height x width</span></span><br><span class="line">mean = X.mean(axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>), keepdims=<span class="keyword">True</span>)</span><br><span class="line">variance = ((X-mean)**<span class="number">2</span>).mean(axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>), keepdims=<span class="keyword">True</span>)</span><br><span class="line"> </span><br><span class="line">eps = <span class="number">1e-5</span></span><br><span class="line">gamma = nd.array([<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">beta = nd.array([<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">X_hat = (X - mean) / nd.sqrt(variance + eps)</span><br><span class="line">y = gamma.reshape(mean.shape) * X_hat + beta.reshape(mean.shape)</span><br></pre></td></tr></table></figure>
<ul>
<li>在给训练数据的计算均值和方差时，同时更新全局的均值和方差，用 <code>moving_momentum=0.9</code> 来限制平均的度，在测试集上，用训练集的均值和方差来归一化：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">moving_mean = moving_mean.reshape(mean.shape)</span><br><span class="line">moving_variance = moving_variance.reshape(mean.shape)</span><br><span class="line">moving_mean[:] = moving_momentum * moving_mean + (<span class="number">1.0</span> - moving_momentum) * mean</span><br><span class="line">moving_variance[:] = moving_momentum * moving_variance + (<span class="number">1.0</span> - moving_momentum) * variance</span><br></pre></td></tr></table></figure>
<ul>
<li>批量归一化在定义模型的时候通过添加 BN 层来实现：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"> </span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    <span class="comment"># 第一层卷积</span></span><br><span class="line">    net.add(nn.Conv2D(channels=<span class="number">20</span>, kernel_size=<span class="number">5</span>))</span><br><span class="line">    <span class="comment">### 添加了批量归一化层</span></span><br><span class="line">    net.add(nn.BatchNorm(axis=<span class="number">1</span>))</span><br><span class="line">    net.add(nn.Activation(activation=<span class="string">'relu'</span>))</span><br><span class="line">    net.add(nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>))</span><br><span class="line">    <span class="comment"># 第二层卷积</span></span><br><span class="line">    net.add(nn.Conv2D(channels=<span class="number">50</span>, kernel_size=<span class="number">3</span>))</span><br><span class="line">    <span class="comment">### 添加了批量归一化层</span></span><br><span class="line">    net.add(nn.BatchNorm(axis=<span class="number">1</span>))</span><br><span class="line">    net.add(nn.Activation(activation=<span class="string">'relu'</span>))</span><br><span class="line">    net.add(nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>))</span><br><span class="line">    net.add(nn.Flatten())</span><br><span class="line">    <span class="comment"># 第一层全连接</span></span><br><span class="line">    net.add(nn.Dense(<span class="number">128</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    <span class="comment"># 第二层全连接</span></span><br><span class="line">    net.add(nn.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<h2 id="4-各种CNN网络结构"><a href="#4-各种CNN网络结构" class="headerlink" title="4. 各种CNN网络结构"></a>4. 各种CNN网络结构</h2><h3 id="4-1-卷积神经网络"><a href="#4-1-卷积神经网络" class="headerlink" title="4.1 卷积神经网络"></a>4.1 卷积神经网络</h3><h4 id="4-1-1-卷积层"><a href="#4-1-1-卷积层" class="headerlink" title="4.1.1 卷积层"></a>4.1.1 卷积层</h4><ul>
<li>当输入数据有多个通道的时候，每个通道会有对应的权重，对每个通道做卷积之后在通道之间求和：</li>
</ul>
<p>$$conv(data, w, b) = \sum\limits_i conv(data[:,i,:,:], w[:,i,:,:], b)$$</p>
<ul>
<li>当输出需要多通道时，每个输出通道有对应权重，然后每个通道上做卷积：</li>
</ul>
<p>$$conv(data, w, b)[:,i,:,:] = conv(data, w[i,:,:,:], b[i])$$</p>
<ul>
<li><code>nd.Convolution</code> 的实现：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="comment"># 输入输出数据格式是 batch x channel x height x width，这里batch和channel都是1</span></span><br><span class="line"><span class="comment"># 权重：output_channels x in_channels x height x width，这里input_filter和output_filter都是1。</span></span><br><span class="line">w = nd.arange(<span class="number">4</span>).reshape((<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)); b = nd.array([<span class="number">1</span>])</span><br><span class="line">data = nd.arange(<span class="number">9</span>).reshape((<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">out = nd.Convolution(data, w, b, kernel=w.shape[<span class="number">2</span>:], num_filter=w.shape[<span class="number">1</span>]) <span class="comment">#输出：[1,1,2,2]</span></span><br><span class="line">out = nd.Convolution(data, w, b, kernel=w.shape[<span class="number">2</span>:], num_filter=w.shape[<span class="number">1</span>],</span><br><span class="line">                     stride=(<span class="number">2</span>,<span class="number">2</span>), pad=(<span class="number">1</span>,<span class="number">1</span>)) <span class="comment"># 输出：[1,1,2,2]</span></span><br><span class="line"><span class="comment">## 多个输入通道</span></span><br><span class="line">w = nd.arange(<span class="number">8</span>).reshape((<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">data = nd.arange(<span class="number">18</span>).reshape((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">out = nd.Convolution(data, w, b, kernel=w.shape[<span class="number">2</span>:], num_filter=w.shape[<span class="number">0</span>]) <span class="comment">#输出：[1,1,2,2]</span></span><br><span class="line"><span class="comment">## 多个输出通道</span></span><br><span class="line">w = nd.arange(<span class="number">16</span>).reshape((<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)); b = nd.array([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">data = nd.arange(<span class="number">18</span>).reshape((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">out = nd.Convolution(data, w, b, kernel=w.shape[<span class="number">2</span>:], num_filter=w.shape[<span class="number">0</span>]) <span class="comment">#输出：[1,2,2,2]</span></span><br></pre></td></tr></table></figure>
<h4 id="4-2-2-池化层"><a href="#4-2-2-池化层" class="headerlink" title="4.2.2 池化层"></a>4.2.2 池化层</h4><ul>
<li>选出窗口里面最大的元素，或者平均元素作为输出:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out = nd.arange(<span class="number">18</span>).reshape((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">max_pool = nd.Pooling(data=out, pool_type=<span class="string">"max"</span>, kernel=(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">avg_pool = nd.Pooling(data=out, pool_type=<span class="string">"avg"</span>, kernel=(<span class="number">2</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<h3 id="4-2-LeNet"><a href="#4-2-LeNet" class="headerlink" title="4.2 LeNet"></a>4.2 LeNet</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment">## LeNet</span></span><br><span class="line"><span class="comment"># 两个卷积-池化层，两个全连接层</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(</span><br><span class="line">        nn.Conv2D(channels=<span class="number">20</span>, kernel_size=<span class="number">5</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">        nn.Conv2D(channels=<span class="number">50</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>),</span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        nn.Dense(<span class="number">128</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dense(<span class="number">10</span>)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<h3 id="4-3-AlexNet"><a href="#4-3-AlexNet" class="headerlink" title="4.3 AlexNet"></a>4.3 AlexNet</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"> </span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(</span><br><span class="line">        <span class="comment"># 第一阶段 kernel_size=11x11, channels=96</span></span><br><span class="line">        nn.Conv2D(channels=<span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">        <span class="comment"># 第二阶段 kernel_size=5x5, channels=256,</span></span><br><span class="line">        nn.Conv2D(channels=<span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">        <span class="comment"># 第三阶段 kernel_size=3x3, channels=[384, 384, 256]</span></span><br><span class="line">        nn.Conv2D(channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Conv2D(channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Conv2D(channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">        <span class="comment"># 第四阶段</span></span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        nn.Dense(<span class="number">4096</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dropout(<span class="number">.5</span>),</span><br><span class="line">        <span class="comment"># 第五阶段</span></span><br><span class="line">        nn.Dense(<span class="number">4096</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dropout(<span class="number">.5</span>),</span><br><span class="line">        <span class="comment"># 第六阶段</span></span><br><span class="line">        nn.Dense(<span class="number">10</span>)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<h3 id="4-4-VGG"><a href="#4-4-VGG" class="headerlink" title="4.4 VGG"></a>4.4 VGG</h3><ul>
<li>论文：<a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span><span class="params">(num_convs, channels)</span>:</span></span><br><span class="line">    out = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_convs):</span><br><span class="line">        out.add(nn.Conv2D(channels=channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    out.add(nn.MaxPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_stack</span><span class="params">(architecture)</span>:</span></span><br><span class="line">    out = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> (num_convs, channels) <span class="keyword">in</span> architecture:</span><br><span class="line">        out.add(vgg_block(num_convs, channels))</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"><span class="comment">## VGG-11</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line">architecture = ((<span class="number">1</span>,<span class="number">64</span>), (<span class="number">1</span>,<span class="number">128</span>), (<span class="number">2</span>,<span class="number">256</span>), (<span class="number">2</span>,<span class="number">512</span>), (<span class="number">2</span>,<span class="number">512</span>))</span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(</span><br><span class="line">        vgg_stack(architecture),</span><br><span class="line">        nn.Flatten(),</span><br><span class="line">        nn.Dense(<span class="number">4096</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dropout(<span class="number">.5</span>),</span><br><span class="line">        nn.Dense(<span class="number">4096</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dropout(<span class="number">.5</span>),</span><br><span class="line">        nn.Dense(num_outputs))</span><br></pre></td></tr></table></figure>
<h3 id="4-5-NiN"><a href="#4-5-NiN" class="headerlink" title="4.5 NiN"></a>4.5 NiN</h3><ul>
<li>Network in Network：串联数个卷积层块和全连接层块来构建深度网络。</li>
<li>NiN 提出只对通道层做全连接并且像素之间共享权重来解决全连接层有过多的参数的问题。就是说，我们使用kernel大小是1×1的卷积。</li>
<li>1x1 的卷积：实现跨通道的交互和信息整合，进行卷积核通道数的降维和升维，减少网络参数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mlpconv</span><span class="params">(channels, kernel_size, padding, strides=<span class="number">1</span>, max_pooling=True)</span>:</span></span><br><span class="line">    out = nn.Sequential()</span><br><span class="line">    out.add(</span><br><span class="line">        nn.Conv2D(channels=channels, kernel_size=kernel_size,</span><br><span class="line">                  strides=strides, padding=padding, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Conv2D(channels=channels, kernel_size=<span class="number">1</span>,</span><br><span class="line">                  padding=<span class="number">0</span>, strides=<span class="number">1</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">        nn.Conv2D(channels=channels, kernel_size=<span class="number">1</span>,</span><br><span class="line">                  padding=<span class="number">0</span>, strides=<span class="number">1</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    <span class="keyword">if</span> max_pooling:</span><br><span class="line">        out.add(nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<ul>
<li>NiN 的卷积层的参数跟 Alexnet 类似，使用三组不同的设定;除了使用了 1×1 卷积外，NiN 在最后不是使用全连接，而是使用通道数为输出类别个数的<code>mlpconv</code>，外接一个平均池化层来将每个通道里的数值平均成一个标量：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(</span><br><span class="line">        mlpconv(<span class="number">96</span>, <span class="number">11</span>, <span class="number">0</span>, strides=<span class="number">4</span>),</span><br><span class="line">        mlpconv(<span class="number">256</span>, <span class="number">5</span>, <span class="number">2</span>),</span><br><span class="line">        mlpconv(<span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">        nn.Dropout(<span class="number">.5</span>),</span><br><span class="line">        <span class="comment"># 目标类为10类</span></span><br><span class="line">        mlpconv(<span class="number">10</span>, <span class="number">3</span>, <span class="number">1</span>, max_pooling=<span class="keyword">False</span>),</span><br><span class="line">        <span class="comment"># 输入为 batch_size x 10 x 5 x 5, 通过AvgPool2D转成</span></span><br><span class="line">        <span class="comment"># batch_size x 10 x 1 x 1。</span></span><br><span class="line">        nn.AvgPool2D(pool_size=<span class="number">5</span>),</span><br><span class="line">        <span class="comment"># 转成 batch_size x 10</span></span><br><span class="line">        nn.Flatten()</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<ul>
<li>“一卷卷到底最后”再加一个平均池化层。</li>
</ul>
<h3 id="4-6-GoogLeNet"><a href="#4-6-GoogLeNet" class="headerlink" title="4.6 GoogLeNet"></a>4.6 GoogLeNet</h3><ul>
<li>Inception：四个并行卷积层的块</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n1_1, n2_1, n2_3, n3_1, n3_5, n4_1, **kwargs)</span>:</span></span><br><span class="line">        super(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># path 1: conv 1x1</span></span><br><span class="line">        self.p1_conv_1 = nn.Conv2D(n1_1, kernel_size=<span class="number">1</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        <span class="comment"># path 2: conv 1x1 conv 3x3</span></span><br><span class="line">        self.p2_conv_1 = nn.Conv2D(n2_1, kernel_size=<span class="number">1</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.p2_conv_3 = nn.Conv2D(n2_3, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        <span class="comment"># path 3: conv 1x1 conv 5x5</span></span><br><span class="line">        self.p3_conv_1 = nn.Conv2D(n3_1, kernel_size=<span class="number">1</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.p3_conv_5 = nn.Conv2D(n3_5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        <span class="comment"># path 4: pool 3x3 conv 1x1</span></span><br><span class="line">        self.p4_pool_3 = nn.MaxPool2D(pool_size=<span class="number">3</span>, padding=<span class="number">1</span>, strides=<span class="number">1</span>)</span><br><span class="line">        self.p4_conv_1 = nn.Conv2D(n4_1, kernel_size=<span class="number">1</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        p1 = self.p1_conv_1(x)</span><br><span class="line">        p2 = self.p2_conv_3(self.p2_conv_1(x))</span><br><span class="line">        p3 = self.p3_conv_5(self.p3_conv_1(x))</span><br><span class="line">        p4 = self.p4_conv_1(self.p4_pool_3(x))</span><br><span class="line">        <span class="keyword">return</span> nd.concat(p1, p2, p3, p4, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>GoogLeNet:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GoogLeNet</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, verbose=False, **kwargs)</span>:</span></span><br><span class="line">        super(GoogLeNet, self).__init__(**kwargs)</span><br><span class="line">        self.verbose = verbose</span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            <span class="comment"># block 1</span></span><br><span class="line">            b1 = nn.Sequential()</span><br><span class="line">            b1.add(</span><br><span class="line">                nn.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, padding=<span class="number">3</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 2</span></span><br><span class="line">            b2 = nn.Sequential()</span><br><span class="line">            b2.add(</span><br><span class="line">                nn.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                nn.Conv2D(<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 3</span></span><br><span class="line">            b3 = nn.Sequential()</span><br><span class="line">            b3.add(</span><br><span class="line">                Inception(<span class="number">64</span>, <span class="number">96</span>, <span class="number">128</span>, <span class="number">16</span>,<span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">                Inception(<span class="number">128</span>, <span class="number">128</span>, <span class="number">192</span>, <span class="number">32</span>, <span class="number">96</span>, <span class="number">64</span>),</span><br><span class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 4</span></span><br><span class="line">            b4 = nn.Sequential()</span><br><span class="line">            b4.add(</span><br><span class="line">                Inception(<span class="number">192</span>, <span class="number">96</span>, <span class="number">208</span>, <span class="number">16</span>, <span class="number">48</span>, <span class="number">64</span>),</span><br><span class="line">                Inception(<span class="number">160</span>, <span class="number">112</span>, <span class="number">224</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">                Inception(<span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">24</span>, <span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">                Inception(<span class="number">112</span>, <span class="number">144</span>, <span class="number">288</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">                Inception(<span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</span><br><span class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 5</span></span><br><span class="line">            b5 = nn.Sequential()</span><br><span class="line">            b5.add(</span><br><span class="line">                Inception(<span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</span><br><span class="line">                Inception(<span class="number">384</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">48</span>, <span class="number">128</span>, <span class="number">128</span>),</span><br><span class="line">                nn.AvgPool2D(pool_size=<span class="number">2</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 6</span></span><br><span class="line">            b6 = nn.Sequential()</span><br><span class="line">            b6.add(</span><br><span class="line">                nn.Flatten(),</span><br><span class="line">                nn.Dense(num_classes)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># chain blocks together</span></span><br><span class="line">            self.net = nn.Sequential()</span><br><span class="line">            self.net.add(b1, b2, b3, b4, b5, b6)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = x</span><br><span class="line">        <span class="keyword">for</span> i, b <span class="keyword">in</span> enumerate(self.net):</span><br><span class="line">            out = b(out)</span><br><span class="line">            <span class="keyword">if</span> self.verbose:</span><br><span class="line">                print(<span class="string">'Block %d output: %s'</span>%(i+<span class="number">1</span>, out.shape))</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Version1</strong>：<a href="http://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Going Deeper with Convolutions</a></li>
<li><strong>Version2</strong>：<a href="http://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> 加入了 Batch Normalization</li>
<li><strong>Version3</strong>：<a href="http://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">Rethinking the Inception Architecture for Computer Vision</a> 调整了 Inception</li>
<li><strong>Version4</strong>：<a href="http://arxiv.org/abs/1602.07261" target="_blank" rel="noopener">Inception-ResNet and the Impact of Residual Connections on Learning</a> 加入了 Residual Connections</li>
</ul>
<h3 id="4-7-ResNet"><a href="#4-7-ResNet" class="headerlink" title="4.7 ResNet"></a>4.7 ResNet</h3><ul>
<li>Residual 块：ResNet 沿用了 VGG 的那种全用 3×3 卷积，但在卷积和池化层之间加入了批量归一层来加速训练，每次跨层连接跨过两层卷积。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, channels, same_shape=True, **kwargs)</span>:</span></span><br><span class="line">        super(Residual, self).__init__(**kwargs)</span><br><span class="line">        self.same_shape = same_shape</span><br><span class="line">        strides = <span class="number">1</span> <span class="keyword">if</span> same_shape <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">        self.conv1 = nn.Conv2D(channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, strides=strides)</span><br><span class="line">        self.bn1 = nn.BatchNorm()</span><br><span class="line">        self.conv2 = nn.Conv2D(channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> same_shape: <span class="comment">#如果输入的通道数和输出不一样时，使用一个额外的1×1卷积来做通道变化</span></span><br><span class="line">            self.conv3 = nn.Conv2D(channels, kernel_size=<span class="number">1</span>, strides=strides)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = nd.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        out = self.bn2(self.conv2(out))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.same_shape:</span><br><span class="line">            x = self.conv3(x)</span><br><span class="line">        <span class="keyword">return</span> nd.relu(out + x)</span><br></pre></td></tr></table></figure>
<ul>
<li>ResNet-18-v1：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, verbose=False, **kwargs)</span>:</span></span><br><span class="line">        super(ResNet, self).__init__(**kwargs)</span><br><span class="line">        self.verbose = verbose</span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            <span class="comment"># block 1: conv 7x7</span></span><br><span class="line">            b1 = nn.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>)</span><br><span class="line">            <span class="comment"># block 2: pool 3x3 + Residual(64) + Residual(64)</span></span><br><span class="line">            b2 = nn.Sequential()</span><br><span class="line">            b2.add(</span><br><span class="line">                nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>),</span><br><span class="line">                Residual(<span class="number">64</span>),</span><br><span class="line">                Residual(<span class="number">64</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 3: Residual(128) + Residual(128)</span></span><br><span class="line">            b3 = nn.Sequential()</span><br><span class="line">            b3.add(</span><br><span class="line">                Residual(<span class="number">128</span>, same_shape=<span class="keyword">False</span>),</span><br><span class="line">                Residual(<span class="number">128</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 4: Residual(256) + Residual(256)</span></span><br><span class="line">            b4 = nn.Sequential()</span><br><span class="line">            b4.add(</span><br><span class="line">                Residual(<span class="number">256</span>, same_shape=<span class="keyword">False</span>),</span><br><span class="line">                Residual(<span class="number">256</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 5: Residual(512) + Residual(512)</span></span><br><span class="line">            b5 = nn.Sequential()</span><br><span class="line">            b5.add(</span><br><span class="line">                Residual(<span class="number">512</span>, same_shape=<span class="keyword">False</span>),</span><br><span class="line">                Residual(<span class="number">512</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># block 6: pool 3x3 + dense</span></span><br><span class="line">            b6 = nn.Sequential()</span><br><span class="line">            b6.add(</span><br><span class="line">                nn.AvgPool2D(pool_size=<span class="number">3</span>),</span><br><span class="line">                nn.Dense(num_classes)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># chain all blocks together</span></span><br><span class="line">            self.net = nn.Sequential()</span><br><span class="line">            self.net.add(b1, b2, b3, b4, b5, b6)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = x</span><br><span class="line">        <span class="keyword">for</span> i, b <span class="keyword">in</span> enumerate(self.net):</span><br><span class="line">            out = b(out)</span><br><span class="line">            <span class="keyword">if</span> self.verbose:</span><br><span class="line">                print(<span class="string">'Block %d output: %s'</span>%(i+<span class="number">1</span>, out.shape))</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<ul>
<li>ResNet_v1：<code>conv-&gt;BN-&gt;relu</code></li>
<li>ResNet_v1_bottleneck：</li>
<li>ResNet_v2：<code>BN-&gt;relu-&gt;conv</code> <a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="noopener">Identity Mappings in Deep Residual Networks</a></li>
<li>ResNet-18/34/50/101/152 一览：<a href="http://lanbing510.info/2017/08/21/ResNet-Keras.html" target="_blank" rel="noopener">ResNet的理解及其Keras实现</a></li>
</ul>
<h3 id="4-8-DenseNet"><a href="#4-8-DenseNet" class="headerlink" title="4.8 DenseNet"></a>4.8 DenseNet</h3><ul>
<li>DenseNet 里来自跳层的输出不是通过加法（+）而是拼接（concat）来跟目前层的输出合并。因为是拼接，所以底层的输出会保留的进入上面所有层。</li>
<li>Dense 块：使用 ResNet 改进版本的 <code>BN-&gt;Relu-&gt;Conv</code> ，每个卷积的输出通道数被称之为 growth_rate，这是因为假设输出为 in_channels，而且有 layers 层，那么输出的通道数就是 <code>in_channels+growth_rate*layers</code>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span><span class="params">(channels)</span>:</span></span><br><span class="line">    out = nn.Sequential()</span><br><span class="line">    out.add(</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'relu'</span>),</span><br><span class="line">        nn.Conv2D(channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layers, growth_rate, **kwargs)</span>:</span></span><br><span class="line">        super(DenseBlock, self).__init__(**kwargs)</span><br><span class="line">        self.net = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(layers):</span><br><span class="line">            self.net.add(conv_block(growth_rate))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.net:</span><br><span class="line">            out = layer(x)</span><br><span class="line">            x = nd.concat(x, out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li>过渡块（Transition Block）：因为使用拼接的缘故，每经过一次拼接输出通道数可能会激增。为了控制模型复杂度，这里引入一个过渡块，它不仅把输入的长宽减半，同时也使用 1×1 卷积来改变通道数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span><span class="params">(channels)</span>:</span></span><br><span class="line">    out = nn.Sequential()</span><br><span class="line">    out.add(</span><br><span class="line">        nn.BatchNorm(),</span><br><span class="line">        nn.Activation(<span class="string">'relu'</span>),</span><br><span class="line">        nn.Conv2D(channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.AvgPool2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<ul>
<li>DenseNet-121：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">init_channels = <span class="number">64</span></span><br><span class="line">growth_rate = <span class="number">32</span></span><br><span class="line">block_layers = [<span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">16</span>]</span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_net</span><span class="params">()</span>:</span></span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">with</span> net.name_scope():</span><br><span class="line">        <span class="comment"># first block: conv 7x7 + pool 3x3</span></span><br><span class="line">        net.add(</span><br><span class="line">            nn.Conv2D(init_channels, kernel_size=<span class="number">7</span>,</span><br><span class="line">                      strides=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">            nn.BatchNorm(),</span><br><span class="line">            nn.Activation(<span class="string">'relu'</span>),</span><br><span class="line">            nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># dense blocks: DenseBlock + transition_block</span></span><br><span class="line">        channels = init_channels</span><br><span class="line">        <span class="keyword">for</span> i, layers <span class="keyword">in</span> enumerate(block_layers):</span><br><span class="line">            net.add(DenseBlock(layers, growth_rate))</span><br><span class="line">            channels += layers * growth_rate</span><br><span class="line">            <span class="keyword">if</span> i != len(block_layers)<span class="number">-1</span>:</span><br><span class="line">                net.add(transition_block(channels//<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># last block: pool + dense</span></span><br><span class="line">        net.add(</span><br><span class="line">            nn.BatchNorm(),</span><br><span class="line">            nn.Activation(<span class="string">'relu'</span>),</span><br><span class="line">            nn.AvgPool2D(pool_size=<span class="number">1</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Dense(num_classes)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure>
<ul>
<li>Desnet 通过将 ResNet 里的 + 替换成 concat 从而获得更稠密的连接。</li>
</ul>
<h2 id="5-Gluon-基础"><a href="#5-Gluon-基础" class="headerlink" title="5. Gluon 基础"></a>5. Gluon 基础</h2><h3 id="5-1-自定义神经网络"><a href="#5-1-自定义神经网络" class="headerlink" title="5.1 自定义神经网络"></a>5.1 自定义神经网络</h3><ul>
<li><code>nn.Sequential</code>的主要好处是定义网络起来更加简单。但<code>nn.Block</code>可以提供更加灵活的网络定义。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 简单神经网络的定义</span></span><br><span class="line"><span class="comment">#通过nn.Sequential</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    net.add(nn.Dense(<span class="number">10</span>))</span><br><span class="line"><span class="comment">#通过继承nn.Block类，定义__init__和forward基本方法</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span> <span class="comment">#参数可以传入prefix和params</span></span><br><span class="line">        super(MLP, self).__init__(**kwargs) <span class="comment">#调用nn.Block的__init__</span></span><br><span class="line">        <span class="keyword">with</span> self.name_scope(): <span class="comment">#调用nn.Block提供的name_scope()</span></span><br><span class="line">            self.dense0 = nn.Dense(<span class="number">256</span>)</span><br><span class="line">            self.dense1 = nn.Dense(<span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.dense1(nd.relu(self.dense0(x)))</span><br><span class="line">net = MLP()</span><br><span class="line"><span class="comment">#nn.Sequential的简单定义</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySequential</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(MySequential, self).__init__(**kwargs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, block)</span>:</span></span><br><span class="line">        self._children.append(block)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._children:</span><br><span class="line">            x = block(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net4 = MySequential()</span><br><span class="line"><span class="keyword">with</span> net4.name_scope():</span><br><span class="line">    net4.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    net4.add(nn.Dense(<span class="number">10</span>))</span><br><span class="line"><span class="comment">#使用nn.Block更灵活的定义</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FancyMLP</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(FancyMLP, self).__init__(**kwargs)</span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            self.dense = nn.Dense(<span class="number">256</span>)</span><br><span class="line">            self.weight = nd.random_uniform(shape=(<span class="number">256</span>,<span class="number">20</span>))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = nd.relu(self.dense(x))</span><br><span class="line">        x = nd.relu(nd.dot(x, self.weight)+<span class="number">1</span>)</span><br><span class="line">        x = nd.relu(self.dense(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li><code>nn</code>下面的类基本都是<code>nn.Block</code>的子类，他们可以很方便地嵌套使用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#nn.Block与nn.Sequential嵌套使用</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RecMLP</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(RecMLP, self).__init__(**kwargs)</span><br><span class="line">        self.net = nn.Sequential()</span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            self.net.add(nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">            self.net.add(nn.Dense(<span class="number">128</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">            self.dense = nn.Dense(<span class="number">64</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> nd.relu(self.dense(self.net(x)))</span><br><span class="line">rec_mlp = nn.Sequential()</span><br><span class="line">rec_mlp.add(RecMLP())</span><br><span class="line">rec_mlp.add(nn.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>设计自定义层</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 不需要维护模型参数的简单自定义层</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CenteredLayer</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(CenteredLayer, self).__init__(**kwargs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x - x.mean()</span><br><span class="line">layer = CenteredLayer()</span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(nn.Dense(<span class="number">128</span>))</span><br><span class="line">    net.add(nn.Dense(<span class="number">10</span>))</span><br><span class="line">    net.add(layer)</span><br><span class="line"><span class="comment">## 带模型参数的自定义层</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDense</span><span class="params">(nn.Block)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units, in_units, **kwargs)</span>:</span></span><br><span class="line">        super(MyDense, self).__init__(**kwargs)</span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            self.weight = self.params.get(<span class="string">'weight'</span>, shape=(in_units, units))</span><br><span class="line">            self.bias = self.params.get(<span class="string">'bias'</span>, shape=(units,))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        linear = nd.dot(x, self.weight.data()) + self.bias.data()</span><br><span class="line">        <span class="keyword">return</span> nd.relu(linear)</span><br><span class="line">dense = MyDense(<span class="number">5</span>, in_units=<span class="number">10</span>, prefix=<span class="string">'o_my_dense_'</span>)</span><br><span class="line">dense.params</span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(MyDense(<span class="number">32</span>, in_units=<span class="number">64</span>))</span><br><span class="line">    net.add(MyDense(<span class="number">2</span>, in_units=<span class="number">32</span>))</span><br></pre></td></tr></table></figure>
<h3 id="5-2-模型参数访问与初始化"><a href="#5-2-模型参数访问与初始化" class="headerlink" title="5.2 模型参数访问与初始化"></a>5.2 模型参数访问与初始化</h3><ul>
<li><a href="https://mxnet.incubator.apache.org/api/python/optimization.html#the-mxnet-initializer-package" target="_blank" rel="noopener">mxnet.initializer</a></li>
<li>延后初始化，为了模型定义的时候不需要指定输入大小。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 模型参数访问</span></span><br><span class="line"><span class="comment">#通过weight和bias访问Dense的参数</span></span><br><span class="line">w = net[<span class="number">0</span>].weight</span><br><span class="line">b = net[<span class="number">0</span>].bias</span><br><span class="line">w.data(); b.data() <span class="comment">#访问参数</span></span><br><span class="line">w.grad(); b.grad() <span class="comment">#访问梯度</span></span><br><span class="line"><span class="comment">#通过collect_params来访问Block里面所有的参数</span></span><br><span class="line">params = net.collect_params() <span class="comment">#会返回一个名字到对应Parameter的dict</span></span><br><span class="line">params[<span class="string">'sequential0_dense0_bias'</span>].data()</span><br><span class="line">params.get(<span class="string">'dense0_weight'</span>).data() <span class="comment">#使用get不需要填写名字前缀</span></span><br><span class="line"><span class="comment">## 模型参数初始化</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line">net.initialize() <span class="comment">#把所有权重初始化成在[-0.07, 0.07]之间均匀分布的随机数</span></span><br><span class="line">params.initialize(init=init.Normal(sigma=<span class="number">0.02</span>), force_reinit=<span class="keyword">True</span>) <span class="comment">#正态分布初始化</span></span><br><span class="line"><span class="comment">## 共享模型参数</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(nn.Dense(<span class="number">4</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    net.add(nn.Dense(<span class="number">4</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    net.add(nn.Dense(<span class="number">4</span>, activation=<span class="string">"relu"</span>, params=net[<span class="number">-1</span>].params))</span><br><span class="line">    net.add(nn.Dense(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>自定义初始化方法</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyInit</span><span class="params">(init.Initializer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyInit, self).__init__()</span><br><span class="line">        self._verbose = <span class="keyword">True</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_weight</span><span class="params">(self, _, arr)</span>:</span></span><br><span class="line">        <span class="comment"># 初始化权重，使用out=arr后我们不需指定形状</span></span><br><span class="line">        print(<span class="string">'init weight'</span>, arr.shape)</span><br><span class="line">        nd.random.uniform(low=<span class="number">5</span>, high=<span class="number">10</span>, out=arr)</span><br><span class="line">net = get_net()</span><br><span class="line">net.initialize(MyInit())</span><br><span class="line">net(x) <span class="comment">#因为延后初始化，所以要调用一次才会初始化</span></span><br><span class="line">net[<span class="number">0</span>].weight.data()</span><br></pre></td></tr></table></figure>
<ul>
<li><code>net.initialize(ctx=mx.gpu(), init=init.Xavier())</code></li>
</ul>
<h3 id="5-3-序列化-读写模型"><a href="#5-3-序列化-读写模型" class="headerlink" title="5.3 序列化 - 读写模型"></a>5.3 序列化 - 读写模型</h3><ul>
<li>读写 NDArray</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line">x = nd.ones(<span class="number">3</span>)</span><br><span class="line">y = nd.zeros(<span class="number">4</span>)</span><br><span class="line">filename = <span class="string">"../data/test1.params"</span></span><br><span class="line">nd.save(filename, [x, y]) <span class="comment">#save写入文件</span></span><br><span class="line">a, b = nd.load(filename) <span class="comment">#load读入文件</span></span><br></pre></td></tr></table></figure>
<ul>
<li>读写Gluon模型的参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line">net = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(nn.Dense(<span class="number">10</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    net.add(nn.Dense(<span class="number">2</span>))</span><br><span class="line">net.initialize()</span><br><span class="line">x = nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">10</span>))</span><br><span class="line">net(x)</span><br><span class="line">filename = <span class="string">"../data/mlp.params"</span></span><br><span class="line">net.save_params(filename) <span class="comment">#save_params写入模型参数</span></span><br><span class="line"> </span><br><span class="line">net2 = nn.Sequential()</span><br><span class="line"><span class="keyword">with</span> net2.name_scope():</span><br><span class="line">    net2.add(nn.Dense(<span class="number">10</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">    net2.add(nn.Dense(<span class="number">2</span>))</span><br><span class="line">net2.load_params(filename) <span class="comment">#load_params读入相同模型的参数</span></span><br></pre></td></tr></table></figure>
<h3 id="5-4-GPU的使用"><a href="#5-4-GPU的使用" class="headerlink" title="5.4 GPU的使用"></a>5.4 GPU的使用</h3><ul>
<li>MXNet 使用 Context 来指定使用哪个设备来存储和计算。默认会将数据开在主内存，然后利用 CPU 来计算，这个由 <code>mx.cpu()</code> 来表示。</li>
<li>GPU 则由 <code>mx.gpu()</code> 来表示。注意 <code>mx.cpu()</code> 表示所有的物理 CPU 和内存，意味着计算上会尽量使用多有的 CPU 核。但 <code>mx.gpu()</code> 只代表一块显卡和其对应的显卡内存。</li>
<li>如果有多块 GPU，我们用 <code>mx.gpu(i)</code> 来表示第 <em>i</em> 块 GPU（ <em>i</em> 从 0 开始）。</li>
<li>GPU 上创建内存：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"> </span><br><span class="line">x = nd.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">x.context <span class="comment">#查看数据存在哪个设备上，默认在CPU</span></span><br><span class="line">a = nd.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], ctx=mx.gpu()) <span class="comment">#指定在GPU上创建变量</span></span><br><span class="line">b = nd.zeros((<span class="number">3</span>,<span class="number">2</span>), ctx=mx.gpu())</span><br><span class="line">c = nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">3</span>), ctx=mx.gpu())</span><br><span class="line">y = x.copyto(mx.gpu()) <span class="comment">#将x传输到GPU上新建内存y上</span></span><br><span class="line">z = x.as_in_context(mx.gpu()) <span class="comment">#相比较copyto的优点在于，如果设备一致，则不复制</span></span><br></pre></td></tr></table></figure>
<ul>
<li>所有计算要求输入数据在同一个设备上。不一致的时候系统不进行自动复制。</li>
<li>默认会复制回 CPU 的操作：如果某个操作需要将 NDArray 里面的内容转出来，例如打印或变成 numpy 格式，如果需要的话系统都会自动将数据 copy 到主内存。</li>
<li>Gluon 的 GPU 计算：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"> </span><br><span class="line">net = gluon.nn.Sequential()</span><br><span class="line">net.add(gluon.nn.Dense(<span class="number">1</span>))</span><br><span class="line">net.initialize(ctx=mx.gpu()) <span class="comment">#模型参数在GPU上初始化</span></span><br><span class="line">data = nd.random.uniform(shape=[<span class="number">3</span>,<span class="number">2</span>], ctx=mx.gpu()) <span class="comment">#GPU上的数据</span></span><br><span class="line">net(data) <span class="comment">#在GPU上计算结果</span></span><br></pre></td></tr></table></figure>
<h2 id="6-优化算法"><a href="#6-优化算法" class="headerlink" title="6. 优化算法"></a>6. 优化算法</h2><ul>
<li>局部最小值：各个方向的梯度都为 0，但不是全局最优点。</li>
<li>鞍点：大部分方向的梯度为 0，其他方向仍有明显下降趋势。</li>
</ul>
<h3 id="6-1-SGD"><a href="#6-1-SGD" class="headerlink" title="6.1 SGD"></a>6.1 SGD</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.2</span></span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: lr&#125;)</span><br><span class="line"><span class="comment"># 学习率在2个epoch后自我衰减</span></span><br><span class="line"><span class="keyword">if</span> epoch &gt; <span class="number">2</span>:</span><br><span class="line">    trainer.set_learning_rate(trainer.learning_rate * <span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="6-2-Momentum"><a href="#6-2-Momentum" class="headerlink" title="6.2 Momentum"></a>6.2 Momentum</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.2</span>; mom = <span class="number">0.9</span></span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: lr, <span class="string">'momentum'</span>: mom&#125;)</span><br><span class="line"><span class="comment"># 重设学习率。</span></span><br><span class="line"><span class="keyword">if</span> epoch &gt; <span class="number">2</span>:</span><br><span class="line">  trainer.set_learning_rate(trainer.learning_rate * <span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="6-3-Adagrad"><a href="#6-3-Adagrad" class="headerlink" title="6.3 Adagrad"></a>6.3 Adagrad</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.2</span></span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'adagrad'</span>, &#123;<span class="string">'learning_rate'</span>: lr&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="6-4-RMSProp"><a href="#6-4-RMSProp" class="headerlink" title="6.4 RMSProp"></a>6.4 RMSProp</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span>; gamma = <span class="number">0.999</span></span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'rmsprop'</span>, &#123;<span class="string">'learning_rate'</span>: lr, <span class="string">'gamma1'</span>: gamma&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="6-5-Adadelta"><a href="#6-5-Adadelta" class="headerlink" title="6.5 Adadelta"></a>6.5 Adadelta</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rho = <span class="number">0.9999</span></span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'adadelta'</span>, &#123;<span class="string">'rho'</span>: rho&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="6-6-Adam"><a href="#6-6-Adam" class="headerlink" title="6.6 Adam"></a>6.6 Adam</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'adam'</span>, &#123;<span class="string">'learning_rate'</span>: lr&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="7-进阶操作"><a href="#7-进阶操作" class="headerlink" title="7. 进阶操作"></a>7. 进阶操作</h2><h3 id="7-1-Hybridize"><a href="#7-1-Hybridize" class="headerlink" title="7.1 Hybridize"></a>7.1 Hybridize</h3><ul>
<li>命令式编程：可以拿到所有中间变量值。</li>
<li>符号式编程：更加高效而且更容易移植。<ul>
<li>定义计算流程</li>
<li>编译成可执行的程序</li>
<li>给定输入调用编译好的程序</li>
</ul>
</li>
<li>用户应该用纯命令式的方法来使用 Gluon 进行开发和调试。但当需要产品级别的性能和部署的时候，我们可以将代码，至少大部分，转换成符号式来运行。</li>
<li>通过使用 HybridBlock 或者 HybridSequential 来构建神经网络。默认他们跟 Block 和 Sequential 一样使用命令式执行。当我们调用<code>.hybridize()</code>后，系统会转换成符号式来执行。事实上，所有 Gluon 里定义的层全是 HybridBlock，这个意味着大部分的神经网络都可以享受符号式执行的优势。</li>
<li><code>nn.HybridSequential</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"> </span><br><span class="line">net = nn.HybridSequential()</span><br><span class="line"><span class="keyword">with</span> net.name_scope():</span><br><span class="line">    net.add(</span><br><span class="line">        nn.Dense(<span class="number">256</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dense(<span class="number">128</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        nn.Dense(<span class="number">2</span>)</span><br><span class="line">    )</span><br><span class="line">net.initialize()</span><br><span class="line">net.hybridize() <span class="comment"># 2倍速度</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>nn.HybridBlock</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HybridNet</span><span class="params">(nn.HybridBlock)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(HybridNet, self).__init__(**kwargs)</span><br><span class="line">        <span class="keyword">with</span> self.name_scope():</span><br><span class="line">            self.fc1 = nn.Dense(<span class="number">10</span>)</span><br><span class="line">            self.fc2 = nn.Dense(<span class="number">2</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hybrid_forward</span><span class="params">(self, F, x)</span>:</span></span><br><span class="line">        print(F)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        <span class="keyword">return</span> self.fc2(x) </span><br><span class="line">net = HybridNet()</span><br><span class="line">net.initialize()</span><br><span class="line">x = nd.random.normal(shape=(<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">y = net(x) <span class="comment">#module 'mxnet.ndarray'</span></span><br><span class="line"> </span><br><span class="line">net.hybridize()</span><br><span class="line">y = net(x)</span><br><span class="line"><span class="comment">#module 'mxnet.symbol',另外，输入和中间输出都变成了symbol</span></span><br><span class="line">y = net(x)</span><br><span class="line"><span class="comment">#无输出，因为将输入替换成Symbol来构建符号式的程序。</span></span><br><span class="line"><span class="comment">#再运行的时候系统将不再访问Python的代码，而是直接在C++后端执行这个符号式程序。</span></span><br></pre></td></tr></table></figure>
<ul>
<li>MXNet 有一个符号式的 API (symbol) 和命令式的 API (ndarray)。这两个接口里面的函数基本是一致的。系统会根据输入来决定 F 是使用 symbol 还是 ndarray 。</li>
</ul>
<h3 id="7-2-延后执行"><a href="#7-2-延后执行" class="headerlink" title="7.2 延后执行"></a>7.2 延后执行</h3><ul>
<li>延后执行使得系统有更多空间来做性能优化。</li>
<li>推荐每个批量里至少有一个同步函数，例如对损失函数进行评估，来避免将过多任务同时丢进后端系统。</li>
<li>同步函数：<ul>
<li><code>print</code></li>
<li><code>nd.NDArray.wait_to_read()</code></li>
<li><code>nd.waitall()</code></li>
</ul>
</li>
</ul>
<h3 id="7-3-自动并行"><a href="#7-3-自动并行" class="headerlink" title="7.3 自动并行"></a>7.3 自动并行</h3><ul>
<li>MXNet 能够自动并行执行没有数据依赖关系的任务从而提升系统性能。</li>
</ul>
<h3 id="7-4-多-CPU-训练"><a href="#7-4-多-CPU-训练" class="headerlink" title="7.4 多 CPU 训练"></a>7.4 多 CPU 训练</h3><ul>
<li>多设备的初始化：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ctx = [gpu(<span class="number">0</span>), gpu(<span class="number">1</span>)]</span><br><span class="line">net.initialize(ctx=ctx)</span><br></pre></td></tr></table></figure>
<ul>
<li>将数据分割并返回各个设备上的复制：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = nd.random.uniform(shape=(<span class="number">4</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">x_list = gluon.utils.split_and_load(x, ctx)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>gluon.Trainer</code> 里面会被默认执行在多GPU之间复制梯度求和并广播。</li>
</ul>
<h2 id="8-计算机视觉领域的常用操作"><a href="#8-计算机视觉领域的常用操作" class="headerlink" title="8. 计算机视觉领域的常用操作"></a>8. 计算机视觉领域的常用操作</h2><h3 id="8-1-图片增广"><a href="#8-1-图片增广" class="headerlink" title="8.1 图片增广"></a>8.1 图片增广</h3><ul>
<li>常用方法以及注意事项：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</span><br><span class="line"> </span><br><span class="line">img = image.imdecode(open(<span class="string">'../img/cat1.jpg'</span>, <span class="string">'rb'</span>).read())</span><br><span class="line">plt.imshow(img.asnumpy())</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span><span class="params">(img, aug, n=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 转成float，一是因为aug需要float类型数据来方便做变化。</span></span><br><span class="line">    <span class="comment"># 二是这里会有一次copy操作，因为有些aug直接通过改写输入（而不是新建输出）获取性能的提升</span></span><br><span class="line">    X = [aug(img.astype(<span class="string">'float32'</span>)) <span class="keyword">for</span> _ <span class="keyword">in</span> range(n*n)]</span><br><span class="line">    <span class="comment"># 有些aug不保证输入是合法值，所以做一次clip</span></span><br><span class="line">    <span class="comment"># 显示浮点图片时imshow要求输入在[0,1]之间</span></span><br><span class="line">    Y = nd.stack(*X).clip(<span class="number">0</span>,<span class="number">255</span>)/<span class="number">255</span></span><br><span class="line">    utils.show_images(Y, n, n, figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 以.5的概率做翻转</span></span><br><span class="line">aug = image.HorizontalFlipAug(<span class="number">.5</span>); apply(img, aug)</span><br><span class="line"><span class="comment">## 随机裁剪一个块 200 x 200 的区域</span></span><br><span class="line">aug = image.RandomCropAug([<span class="number">200</span>,<span class="number">200</span>]); apply(img, aug)</span><br><span class="line"><span class="comment">## 随机裁剪一块随机大小的区域</span></span><br><span class="line">aug = image.RandomSizedCropAug((<span class="number">200</span>,<span class="number">200</span>), <span class="number">.1</span>, (<span class="number">.5</span>,<span class="number">2</span>)); apply(img, aug)</span><br><span class="line"><span class="comment">## 随机将亮度增加或者减小在0-50%间的一个量</span></span><br><span class="line">aug = image.BrightnessJitterAug(<span class="number">.5</span>); apply(img, aug)</span><br><span class="line"><span class="comment">## 随机色调变化</span></span><br><span class="line">aug = image.HueJitterAug(<span class="number">.5</span>); apply(img, aug)</span><br></pre></td></tr></table></figure>
<ul>
<li>CIFAR10 使用示例：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_aug_list</span><span class="params">(img, augs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> augs: img = f(img)</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_transform</span><span class="params">(augs)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(data, label)</span>:</span></span><br><span class="line">        <span class="comment"># data: sample x height x width x channel label: sample</span></span><br><span class="line">        data = data.astype(<span class="string">'float32'</span>)</span><br><span class="line">        <span class="keyword">if</span> augs <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            data = nd.stack(*[apply_aug_list(d, augs) <span class="keyword">for</span> d <span class="keyword">in</span> data])</span><br><span class="line">        data = nd.transpose(data, (<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>)) <span class="comment">#sample x channel x height x width</span></span><br><span class="line">        <span class="keyword">return</span> data, label.astype(<span class="string">'float32'</span>)</span><br><span class="line">    <span class="keyword">return</span> transform</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(batch_size, train_augs, test_augs=None)</span>:</span></span><br><span class="line">    cifar10_train = gluon.data.vision.CIFAR10(train=<span class="keyword">True</span>, transform=get_transform(train_augs))</span><br><span class="line">    cifar10_test = gluon.data.vision.CIFAR10(train=<span class="keyword">False</span>, transform=get_transform(test_augs))</span><br><span class="line">    train_data = utils.DataLoader(cifar10_train, batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">    test_data = utils.DataLoader(cifar10_test, batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> (train_data, test_data)</span><br></pre></td></tr></table></figure>
<h3 id="8-2-Fine-tuning"><a href="#8-2-Fine-tuning" class="headerlink" title="8.2 Fine-tuning"></a>8.2 Fine-tuning</h3><ul>
<li>Hotdog 使用示例：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="comment">## 获取数据集</span></span><br><span class="line">data_dir = <span class="string">'../data'</span></span><br><span class="line">fname = gluon.utils.download(</span><br><span class="line">    <span class="string">'https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/hotdog.zip'</span>,</span><br><span class="line">    path=data_dir, sha1_hash=<span class="string">'fba480ffa8aa7e0febbb511d181409f899b9baa5'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> zipfile.ZipFile(fname, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.extractall(data_dir)</span><br><span class="line"><span class="comment">## 图片增强</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line">train_augs = [image.HorizontalFlipAug(<span class="number">.5</span>), image.RandomCropAug((<span class="number">224</span>,<span class="number">224</span>))]</span><br><span class="line">test_augs = [image.CenterCropAug((<span class="number">224</span>,<span class="number">224</span>))]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(data, label, augs)</span>:</span></span><br><span class="line">    data = data.astype(<span class="string">'float32'</span>)</span><br><span class="line">    <span class="keyword">for</span> aug <span class="keyword">in</span> augs:</span><br><span class="line">        data = aug(data)</span><br><span class="line">    data = nd.transpose(data, (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> data, nd.array([label]).asscalar().astype(<span class="string">'float32'</span>)</span><br><span class="line">train_imgs = gluon.data.vision.ImageFolderDataset(data_dir+<span class="string">'/hotdog/train'</span>,</span><br><span class="line">    transform=<span class="keyword">lambda</span> X, y: transform(X, y, train_augs))</span><br><span class="line">test_imgs = gluon.data.vision.ImageFolderDataset(data_dir+<span class="string">'/hotdog/test'</span>,</span><br><span class="line">    transform=<span class="keyword">lambda</span> X, y: transform(X, y, test_augs))</span><br><span class="line"><span class="comment">## 读取数据</span></span><br><span class="line">data = gluon.data.DataLoader(train_imgs, <span class="number">32</span>, shuffle=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">## 获取预训练模型</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon.model_zoo <span class="keyword">import</span> vision <span class="keyword">as</span> models</span><br><span class="line">pretrained_net = models.resnet18_v2(pretrained=<span class="keyword">True</span>) <span class="comment">#pretrained_net = features + output</span></span><br><span class="line"><span class="comment">## 模型定义</span></span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line">finetune_net = models.resnet18_v2(classes=<span class="number">2</span>)</span><br><span class="line">finetune_net.features = pretrained_net.features <span class="comment">#features与预训练模型一致</span></span><br><span class="line">finetune_net.output.initialize(init.Xavier()) <span class="comment">#仅初始化output部分</span></span><br><span class="line"><span class="comment">## 训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(net, ctx, batch_size=<span class="number">64</span>, epochs=<span class="number">10</span>, learning_rate=<span class="number">0.01</span>, wd=<span class="number">0.001</span>)</span>:</span></span><br><span class="line">    train_data = gluon.data.DataLoader(train_imgs, batch_size, shuffle=<span class="keyword">True</span>)</span><br><span class="line">    test_data = gluon.data.DataLoader(test_imgs, batch_size)</span><br><span class="line">    net.collect_params().reset_ctx(ctx) <span class="comment">#确保net的初始化在ctx上</span></span><br><span class="line">    net.hybridize()</span><br><span class="line">    loss = gluon.loss.SoftmaxCrossEntropyLoss()</span><br><span class="line">    trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;</span><br><span class="line">        <span class="string">'learning_rate'</span>: learning_rate, <span class="string">'wd'</span>: wd&#125;)</span><br><span class="line">    utils.train(train_data, test_data, net, loss, trainer, ctx, epochs)</span><br></pre></td></tr></table></figure>
<ul>
<li>附录：utils.train() 的定义</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(train_data, test_data, net, loss, trainer, ctx, num_epochs, print_batches=None)</span>:</span></span><br><span class="line">    <span class="string">"""Train a network"""</span></span><br><span class="line">    print(<span class="string">"Start training on "</span>, ctx)</span><br><span class="line">    <span class="keyword">if</span> isinstance(ctx, mx.Context):</span><br><span class="line">        ctx = [ctx]</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_loss, train_acc, n, m = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(train_data, mx.io.MXDataIter):</span><br><span class="line">            train_data.reset()</span><br><span class="line">        start = time()</span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(train_data):</span><br><span class="line">            data, label, batch_size = _get_batch(batch, ctx)</span><br><span class="line">            losses = []</span><br><span class="line">            <span class="keyword">with</span> autograd.record():</span><br><span class="line">                outputs = [net(X) <span class="keyword">for</span> X <span class="keyword">in</span> data]</span><br><span class="line">                losses = [loss(yhat, y) <span class="keyword">for</span> yhat, y <span class="keyword">in</span> zip(outputs, label)]</span><br><span class="line">            <span class="keyword">for</span> l <span class="keyword">in</span> losses:</span><br><span class="line">                l.backward()</span><br><span class="line">            train_acc += sum([(yhat.argmax(axis=<span class="number">1</span>)==y).sum().asscalar()</span><br><span class="line">                              <span class="keyword">for</span> yhat, y <span class="keyword">in</span> zip(outputs, label)])</span><br><span class="line">            train_loss += sum([l.sum().asscalar() <span class="keyword">for</span> l <span class="keyword">in</span> losses])</span><br><span class="line">            trainer.step(batch_size)</span><br><span class="line">            n += batch_size</span><br><span class="line">            m += sum([y.size <span class="keyword">for</span> y <span class="keyword">in</span> label])</span><br><span class="line">            <span class="keyword">if</span> print_batches <span class="keyword">and</span> (i+<span class="number">1</span>) % print_batches == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Batch %d. Loss: %f, Train acc %f"</span> % (n, train_loss/n, train_acc/m))</span><br><span class="line"> </span><br><span class="line">        test_acc = evaluate_accuracy(test_data, net, ctx)</span><br><span class="line">        print(<span class="string">"Epoch %d. Loss: %.3f, Train acc %.2f, Test acc %.2f, Time %.1f sec"</span> % (</span><br><span class="line">            epoch, train_loss/n, train_acc/m, test_acc, time() - start))</span><br></pre></td></tr></table></figure>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ul>
<li><a href="http://zh.gluon.ai/" target="_blank" rel="noopener">动手学深度学习</a></li>
<li><a href="https://discuss.gluon.ai/" target="_blank" rel="noopener">MXNet / Gluon 论坛</a></li>
</ul>
</div><div class="tags"><a href="/tags/MXNet/">MXNet</a><a href="/tags/Gluon/">Gluon</a></div><div class="post-nav"><a href="/2018/01/17/损失函数的推导/" class="pre">损失函数的推导</a><a href="/2018/01/14/IPV4 IPV6 科学上网/" class="next">IPV4 IPV6 科学上网</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><a href="http://dday.top/">
<img src="http://owinowxgh.bkt.clouddn.com/meblack2.jpg" alt="" border="0" style="margin-top:15px; border-radius: 300px;" width="180px"; height="180px"; >
</a></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目整理/">项目整理</a><span class="category-list-count">3</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/tags/MXNet/" style="font-size: 15px;">MXNet</a> <a href="/tags/Gluon/" style="font-size: 15px;">Gluon</a> <a href="/tags/AWS/" style="font-size: 15px;">AWS</a> <a href="/tags/Ubuntu/" style="font-size: 15px;">Ubuntu</a> <a href="/tags/PyTorch/" style="font-size: 15px;">PyTorch</a> <a href="/tags/竞赛/" style="font-size: 15px;">竞赛</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/NexT/" style="font-size: 15px;">NexT</a> <a href="/tags/GitHub/" style="font-size: 15px;">GitHub</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/科学上网/" style="font-size: 15px;">科学上网</a> <a href="/tags/Shadowsocks/" style="font-size: 15px;">Shadowsocks</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/理论/" style="font-size: 15px;">理论</a> <a href="/tags/吴恩达/" style="font-size: 15px;">吴恩达</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/C/" style="font-size: 15px;">C++</a> <a href="/tags/conda/" style="font-size: 15px;">conda</a> <a href="/tags/ffmpeg/" style="font-size: 15px;">ffmpeg</a> <a href="/tags/Windows/" style="font-size: 15px;">Windows</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/01/17/损失函数的推导/">损失函数的推导</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/15/Gluon for MXNet 使用与概念梳理（冗长版）/">Gluon for MXNet 使用与概念梳理（冗长版）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/14/IPV4 IPV6 科学上网/">IPV4 IPV6 科学上网</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/14/激活函数一览/">激活函数一览</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/09/快排，归并，堆排序算法/">快排，归并，堆排序算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/05/终端命令整理/">终端命令整理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/01/01/Kaggle - 120 种狗的图像识别/">Kaggle - 120种狗的图像识别</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/13/AWS 上使用 MXNet 的环境配置/">AWS 上使用 MXNet 的环境配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/15/AI Challenger 场景分类/">AI Challenger 场景分类</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/07/Ubuntu 下 PyTorch 的环境配置/">Ubuntu 下 PyTorch 的环境配置</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">DDay's blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.1.20/jquery.fancybox.min.css"><script type="text/javascript" color="100,99,98" opacity="0.5" zIndex="-2" count="50" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>